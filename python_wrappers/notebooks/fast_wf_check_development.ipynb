{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "sys.path.insert(0,\"/home/daqtest/Processor/sandpro\")\n",
    "import sandpro\n",
    "import configparser\n",
    "import json\n",
    "import scipy.stats\n",
    "from scipy.optimize import curve_fit\n",
    "import datetime\n",
    "import pandas as pd\n",
    "# %run ../WaveformProcessor.py\n",
    "# %run ../FastProcessing.py\n",
    "# %run ../FitSPE.py\n",
    "\n",
    "sys.path.insert(0,\"../src/\")\n",
    "import common.d2d as d2d\n",
    "import common.utils as util\n",
    "import data_processing.fast_processor as fast_processor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which datasets are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/daqtest/DAQ/SandyAQ/softlink_to_data/noise_time_evolution_before_threshold_calibration_implemented/\"\n",
    "# path = \"/home/daqtest/DAQ/SandyAQ/softlink_to_data/all_data/20240819_T102_50V_5.0sig/\"\n",
    "path = \"/home/daqtest/DAQ/SandyAQ/softlink_to_data/all_data/20240624_T100_46V_4.0sig/\"\n",
    "# path = \"/home/daqtest/DAQ/SandyAQ/softlink_to_data/all_data/T100_47_5,5sig_V/\" # check waveform\n",
    "calibration_run = False # True if you want to check the calibration, False otherwise\n",
    "plot = False # if want to replot the waveforms\n",
    "# ignore_channel_list = np.array([2,8,11,14,20,23])\n",
    "ignore_channel_list = np.array([])\n",
    "\n",
    "# ##########################\n",
    "# Load the meta data files\n",
    "if calibration_run:\n",
    "    data_folder = os.path.join(path,\"threshold_calibration/\")\n",
    "else:\n",
    "    data_folder = path\n",
    "\n",
    "meta_data_list = glob.glob(f\"{data_folder}/*meta*\")\n",
    "#reorganize the order of the meta_data_list, so the final one should be the newest one.\n",
    "\n",
    "# Sort the file paths based on the extracted date and time in ascending order (oldest to newest)\n",
    "new_meta_data_list = sorted(meta_data_list, key=util.extract_date_meta_data)\n",
    "new_meta_data_list = sorted(new_meta_data_list[-24:], key=util.extract_channel_meta_data)\n",
    "meta_data_list = new_meta_data_list[-24:]\n",
    "meta_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to remove the PostProcessing legacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastProcessor = fast_processor.FastProcessor(data_folder, meta_data_list, \n",
    "               ignore_channel_list=ignore_channel_list,\n",
    "               calibration_run=calibration_run)\n",
    "FastProcessor.process_runs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "FastProcessor.plot_waveforms(save_plot = False, show_plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/daqtest/DAQ/SandyAQ/softlink_to_data/noise_time_evolution_before_threshold_calibration_implemented/\"\n",
    "# path = \"/home/daqtest/DAQ/SandyAQ/softlink_to_data/all_data/20240628_2_T102_47V_6.0sig/\"\n",
    "path = \"/home/daqtest/DAQ/SandyAQ/softlink_to_data/all_data/T100_47_5,5sig_V/\" # check waveform\n",
    "calibration_run = False # True if you want to check the calibration, False otherwise\n",
    "plot = False # if want to replot the waveforms\n",
    "ignore_channel_list = np.array([2,8,11,14,20,23])\n",
    "\n",
    "# ##########################\n",
    "# Load the meta data files\n",
    "if calibration_run:\n",
    "    data_folder = os.path.join(path,\"threshold_calibration/\")\n",
    "else:\n",
    "    data_folder = path\n",
    "\n",
    "meta_data_list = glob.glob(f\"{data_folder}/*meta*\")\n",
    "#reorganize the order of the meta_data_list, so the final one should be the newest one.\n",
    "\n",
    "# Sort the file paths based on the extracted date and time in ascending order (oldest to newest)\n",
    "new_meta_data_list = sorted(meta_data_list, key=util.extract_date_meta_data)\n",
    "new_meta_data_list = sorted(new_meta_data_list[-24:], key=util.extract_channel_meta_data)\n",
    "meta_data_list=new_meta_data_list[-24:]\n",
    "\n",
    "post_processing = FastProcessing(data_folder, meta_data_list[3], \n",
    "               ignore_channel_list=ignore_channel_list,\n",
    "               calibration_run=calibration_run)\n",
    "\n",
    "spd =  post_processing.process_run()\n",
    "\n",
    "df = post_processing.df\n",
    "\n",
    "randomly_selected_raw_WF = np.transpose(post_processing.df.randomly_selected_raw_WF[0])\n",
    "randomly_selected_filtered_WF = np.transpose(post_processing.df.randomly_selected_filtered_WF[0])\n",
    "baseline_mean = df.baseline_mean[0]\n",
    "data_end = 500\n",
    "raw_WF = randomly_selected_raw_WF[:data_end,20]\n",
    "filtered_WF = randomly_selected_filtered_WF[:data_end,20]\n",
    "time = np.arange(0, data_end, 1) * 4\n",
    "plt.plot(time,raw_WF-baseline_mean)\n",
    "plt.plot(time,filtered_WF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"time[ns]\":time,\n",
    "                   \"raw_waveform[V]\":raw_WF})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"raw_waveform_zepeng.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = post_processing.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist_count, bin_edges, _ = plt.hist(df[df[\"channel\"]==13].areas,bins=200,range=(-0.1,10),histtype='step',color='red')\n",
    "bin_centers = bin_edges[:-1] + np.diff(bin_edges)/2\n",
    "\n",
    "bin_density = 10/len(bin_centers)\n",
    "distance_rough_guess = 0.5 #unit\n",
    "peaks, _ = find_peaks(hist_count, height=5, distance=distance_rough_guess/bin_density)\n",
    "# peaks, _ = find_peaks(n_hist, height=5)\n",
    "\n",
    "plt.plot(bin_centers,hist_count,color='red',label='channel 1')\n",
    "plt.plot(bin_centers[peaks], hist_count[peaks], \"x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spe_fit = FitSPE.FitSPE(hist_count, bin_edges, plot = False, show_plot=False, save_plot=False)\n",
    "np.transpose(spe_fit.line_x)[:,np.array(spe_fit.mu_err_list)<0.02].shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../FitSPE.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = FitSPE(n_hist, bin_edge, plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE_rough_position = bin_centers[peaks] #unit: mV*ns\n",
    "PE_rough_half_width = np.median(np.diff(PE_rough_position))/2\n",
    "PE_rough_amplitude = n_hist[peaks]\n",
    "PE_half_width_index = int(np.median(np.diff(peaks))/2) # PE width in index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to model and create data \n",
    "def func(x, a, x0, sigma): \n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2))\n",
    "\n",
    "fit_which_peak = 0\n",
    "# PE_width_n = int(np.median(np.diff(peaks))/2)\n",
    "# peak=peaks[fit_which_peak]\n",
    "\n",
    "min_x_list = []\n",
    "max_x_list = []\n",
    "\n",
    "plt.plot(bin_centers, n_hist, c='black', label='data') \n",
    "\n",
    "\n",
    "# Executing curve_fit on noisy data \n",
    "\n",
    "for i, peak in enumerate(peaks[0:10]):\n",
    "    min_x = int(peak-PE_half_width_index)\n",
    "    max_x = int(peak+PE_half_width_index)\n",
    "    min_x_list.append(min_x)\n",
    "    max_x_list.append(max_x)\n",
    "    \n",
    "    (amp,mu,sig), pcov = curve_fit(func, bin_centers[min_x:max_x], n_hist[min_x:max_x], p0=[PE_rough_amplitude[i], PE_rough_position[i], PE_rough_half_width])\n",
    "  \n",
    "    ym = func(bin_centers[min_x:max_x], amp,mu,sig) \n",
    "    plt.plot(bin_centers[min_x:max_x], ym, c='r', label='Best fit') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE_rough_half_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_histograms(self, n_hist, bin_edges, plot=True):\n",
    "        \n",
    "        bin_centers = bin_edges[:-1] + np.diff(bin_edges)/2\n",
    "        bin_density = 10/len(bin_centers)\n",
    "        distance_rough_guess = 0.5 # distance between peaks in mV*ns\n",
    "\n",
    "        peaks, _ = find_peaks(n_hist, height=5, distance=distance_rough_guess/bin_density)\n",
    "\n",
    "        PE_rough_position = bin_centers[peaks] # unit: mV*ns\n",
    "        PE_rough_half_width = np.median(np.diff(PE_rough_position))/2\n",
    "        PE_rough_amplitude = n_hist[peaks]\n",
    "        PE_half_width_index = int(np.median(np.diff(peaks))/2) # PE width in index\n",
    "        \n",
    "        # Executing curve_fit on noisy data \n",
    "        for peak in peaks:\n",
    "            min_x = peak - PE_half_width_index\n",
    "            max_x = peak + PE_half_width_index\n",
    "            (amp,mu,sig), _ = curve_fit(self._gaussian_func, \n",
    "                                bin_centers[min_x:max_x], \n",
    "                                n_hist[min_x:max_x], \n",
    "                                p0=[PE_rough_amplitude[fit_which_peak], \n",
    "                                    PE_rough_position[fit_which_peak], \n",
    "                                    PE_rough_half_width])\n",
    "        \n",
    "        #popt returns the best fit values for parameters of the given model (func) \n",
    "        print (amp,mu,sig) \n",
    "        \n",
    "        ym = self._gaussian_func(bin_centers, amp, mu, sig) \n",
    "        plt.plot(bin_centers, n_hist, c='black', label='data') \n",
    "        plt.plot(bin_centers, ym, c='r', label='Best fit') \n",
    "\n",
    "        if plot:\n",
    "            plt.figure()\n",
    "            plt.plot(bin_centers,n_hist,color='red',label='channel 1')\n",
    "            plt.plot(bin_centers[peaks], n_hist[peaks], \"x\")\n",
    "            plt.show()\n",
    "\n",
    "def _gaussian_func(x, a, x0, sigma): \n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run_id = \"1\" # name for output file\n",
    "\n",
    "# # last_n_dataset = 2\n",
    "# # sorted_meta_data_list = meta_data_list[-24*last_n_dataset-1:-24*last_n_dataset+23]\n",
    "# sorted_meta_data_list = meta_data_list[-24:]\n",
    "\n",
    "# ignore_channel_list = np.array([2,8,11,14,20,23])\n",
    "# # for the ignore channel list, we ignore 2,8,11,14,20,23. \n",
    "# # I add a few channels in the list cuz I wanna to do some coding  to #3.\n",
    "\n",
    "# #plot settings\n",
    "# fontsize = \"small\"\n",
    "\n",
    "# baseline_mean_list = []\n",
    "# baseline_std_list = []\n",
    "# mu_all_channel=[]\n",
    "# datetime_list = []\n",
    "# bias_voltage_list = []\n",
    "\n",
    "# for i in range(len(sorted_meta_data_list)):\n",
    "#     print(sorted_meta_data_list[i])\n",
    "#     meta_data = sorted_meta_data_list[i]\n",
    "#     meta_data_basename = os.path.basename(meta_data)\n",
    "#     parts = meta_data_basename.split('_')\n",
    "    \n",
    "#     config_name = \"_\".join(parts[1:4]) + \".ini\"\n",
    "#     threshold_adc = parts[3]\n",
    "#     channel = int(parts[2])\n",
    "#     date_datafile = int(parts[4])\n",
    "#     time_datafile = int(parts[5].split('.')[0])\n",
    "\n",
    "#     datetime_object = datetime.datetime.strptime(f\"{parts[4]} {parts[5].split('.')[0]}\", '%Y%m%d %H%M%S')\n",
    "#     datetime_list.append(datetime_object)\n",
    "\n",
    "#     # ignore the channels that in the ignore_channel_list\n",
    "#     if channel in ignore_channel_list:\n",
    "#         continue\n",
    "\n",
    "#     # read from meta data file\n",
    "#     with open(meta_data, \"r\") as f:\n",
    "#         data_taking_settings = json.load(f)\n",
    "\n",
    "#     n_events = int(data_taking_settings[\"number_of_events\"])\n",
    "#     bias_voltage = float(data_taking_settings[\"voltage_config\"][\"preamp_1\"]) # assuming all preamp has the same bias voltage; can be easily changed\n",
    "#     bias_voltage_list.append(bias_voltage)\n",
    "\n",
    "#     print(config_name)\n",
    "#     config_path = os.path.join(data_folder, \"tmp\",config_name)\n",
    "#     config = configparser.ConfigParser()\n",
    "#     config.optionxform = str\n",
    "#     config.read(config_path)\n",
    "\n",
    "#     process_config = {\"nchs\": 1,\n",
    "#     \"nsamps\": int(config.get(\"COMMON\", \"RECORD_LENGTH\")),\n",
    "#     \"sample_selection\": 120,\n",
    "#     \"samples_to_average\": 40}\n",
    "\n",
    "#     # dump the config to a json file\n",
    "#     with open(\"process_config.json\", \"w\") as f:\n",
    "#         json.dump(process_config, f)\n",
    "#         # set the board number and integral window according to the board number (board 0 and 1 have different integral windows)\n",
    "#     if len(config.get(\"BOARD-0\", \"CHANNEL_LIST\")) > 0:\n",
    "#         board_number = 0\n",
    "#         local_channel = channel\n",
    "#         integral_window = (0.3,0.55)\n",
    "#     else:\n",
    "#         board_number = 1\n",
    "#         integral_window = (0.4,0.6)\n",
    "#         local_channel = channel - 16\n",
    "\n",
    "#     processor= sandpro.processing.rawdata.RawData(config_file = \"process_config.json\",\n",
    "#                                                perchannel=False)\n",
    "#     data_file_basename = meta_data_basename.replace(\"meta_\", \"\").replace(\".json\", f\"_board_{board_number}.bin\")\n",
    "\n",
    "#     # section_name = f\"BOARD-{board_number}_CHANNEL-{local_channel}\"\n",
    "#     # option_name = \"N_EVENTS\"\n",
    "#     # if config.has_option(section_name, option_name):\n",
    "#     # if n_events > :\n",
    "#         # n_events = int(config.get(section_name, option_name))\n",
    "#         # data = processor.get_rawdata_numpy(n_evts=int(n_events)-1,\n",
    "#         #                             file=os.path.join(data_folder, data_file_basename),\n",
    "#         #                             bit_of_daq=14,\n",
    "#         #                             headersize=4,inversion=False)\n",
    "#         # # start_index, end_index = 1500, 1980\n",
    "#         # start_index, end_index = 1000, int(n_events)-1\n",
    "#         # print(f\"analysing events from {start_index} to {end_index}\")\n",
    "#     # else:\n",
    "#     try:\n",
    "#         data = processor.get_rawdata_numpy(n_evts=n_events-1,\n",
    "#                                     file=os.path.join(data_folder, data_file_basename),\n",
    "#                                     bit_of_daq=14,\n",
    "#                                     headersize=4,inversion=False)\n",
    "#         # start_index, end_index = 1500, 1980\n",
    "#         start_index, end_index = 1000, n_events-1-500\n",
    "#         print(f\"analysing events from range: {start_index} to {end_index}\")\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         data = processor.get_rawdata_numpy(1999,\n",
    "#                                     file=os.path.join(data_folder, data_file_basename),\n",
    "#                                     bit_of_daq=14,\n",
    "#                                     headersize=4,inversion=False)\n",
    "#         start_index, end_index = 1000, 1999\n",
    "\n",
    "\n",
    "#     wfp = WFProcessor(data_folder, volt_per_adc=2/2**14)\n",
    "#     wfp.set_data(data[\"data_per_channel\"][start_index:end_index,0], in_adc = False)\n",
    "#     wfp.process_wfs()\n",
    "    \n",
    "#     baseline_std = np.mean(wfp.baseline_rms)\n",
    "#     baseline_mean = np.mean(wfp.baseline)\n",
    "#     baseline_mean_list.append(baseline_mean)\n",
    "#     baseline_std_list.append(baseline_std)\n",
    "#     n_processed_events = len(wfp.baseline_rms)\n",
    "\n",
    "#     # print(\"number of events: \", len(wfp.baseline_rms))\n",
    "\n",
    "#     threshold_mv = util.adc_to_mv(int(threshold_adc))\n",
    "#     # set the threshold to be 2 * Vpp\n",
    "#     optimal_threshold_mv =  (baseline_mean + 2 * (2 * np.sqrt(2) * baseline_std)) * 1000\n",
    "#     optimal_threshold_3sig_mv =  (baseline_mean + 3 * baseline_std) * 1000\n",
    "#     optimal_threshold_adc = util.mv_to_adc(optimal_threshold_mv)\n",
    "#     areas = wfp.get_area(sum_window=integral_window)\n",
    "#     heights = wfp.get_height(search_window=integral_window)\n",
    "\n",
    "#     print(\"thresholds, opt, thres: \",optimal_threshold_adc, threshold_adc)\n",
    "\n",
    "#     def gaussian(x, amplitude, mean, sigma):\n",
    "#         return amplitude * np.exp(-((x - mean) / sigma) ** 2 / 2)\n",
    "#     # we will use the gaussian fit later\n",
    "\n",
    "#     if plot:\n",
    "#         figure, axes = plt.subplots(3,2,figsize=(15,15))\n",
    "#         # figure.subplots_adjust(wspace=0.5)\n",
    "#         # figure.subplots_adjust(hspace=0.5)\n",
    "#         time = np.arange(0, process_config[\"nsamps\"], 1) * 4\n",
    "\n",
    "#         data_processed = data[\"data_per_channel\"][start_index:end_index,0,:]\n",
    "#         mask = np.random.rand(n_processed_events) < 0.05\n",
    "#         data_processed[mask,:]\n",
    "#         axes[0,0].plot(time, 1000 * np.transpose(data_processed[mask,:]),color=\"red\",alpha=0.2)\n",
    "#         axes[1,0].plot(time, 1000 * np.transpose(data_processed[mask,:]),color=\"red\",alpha=0.2)\n",
    "#         i -= start_index\n",
    "#         axes[0,1].plot(time, 1000 * np.transpose(wfp.filtered_wfs[mask,:]),color=\"red\",alpha=0.2)\n",
    "#         axes[1,1].plot(time, 1000 * np.transpose(wfp.filtered_wfs[mask,:]),color=\"red\",alpha=0.2)\n",
    "        \n",
    "#         # forloop is slower\n",
    "#         # for i in range(start_index, end_index):\n",
    "#         #     # print 5% random waveforms within the index\n",
    "#         #     if np.random.rand() < 0.05:\n",
    "#         #         axes[0,0].plot(time, 1000 * data[\"data_per_channel\"][i][0],color=\"red\",alpha=0.2)\n",
    "#         #         axes[1,0].plot(time, 1000 * data[\"data_per_channel\"][i][0],color=\"red\",alpha=0.2)\n",
    "#         #         i -= start_index\n",
    "#         #         axes[0,1].plot(time, 1000 * wfp.filtered_wfs[i],color=\"red\",alpha=0.2)\n",
    "#         #         axes[1,1].plot(time, 1000 * wfp.filtered_wfs[i],color=\"red\",alpha=0.2)\n",
    "#         axes[0,0].axhline(optimal_threshold_mv, color='g',label=f\"Optimal threshold: \\n{int(optimal_threshold_adc)}[ADC] \\n{int(optimal_threshold_mv)}[mV]\")\n",
    "#         axes[0,0].axhline(optimal_threshold_3sig_mv, color='g',label=f\"3 sig threshold: \\n{int(optimal_threshold_mv)}[mV]\")\n",
    "#         axes[0,0].axhline(threshold_mv, color='b',label=f\"Test threshold: \\n{int(threshold_adc)}[ADC] \\n{int(threshold_mv)}[mV]\", zorder=10)\n",
    "#         axes[0,0].set_xlim(0,3900)\n",
    "\n",
    "#         axes[1,0].axhline(optimal_threshold_mv, color='g',label=f\"Optimal threshold: \\n{int(optimal_threshold_adc)}[ADC] \\n{int(optimal_threshold_mv)}[mV]\")\n",
    "#         axes[1,0].axhline(optimal_threshold_3sig_mv, color='g',label=f\"3 sig threshold: \\n{int(optimal_threshold_mv)}[mV]\")\n",
    "#         axes[1,0].axhline(threshold_mv, color='b',label=f\"Test threshold: \\n{int(threshold_adc)}[ADC] \\n{int(threshold_mv)}[mV]\", zorder=10)\n",
    "#         axes[1,0].set_xlim(0,3900)\n",
    "\n",
    "#         _ymax = 1800\n",
    "#         axes[0,0].set_ylim(200,_ymax)\n",
    "#         axes[0,0].text(100,optimal_threshold_mv + _ymax/5,f\"baseline mean: {1000 * baseline_mean:.1f} mV\")\n",
    "#         axes[0,0].text(100,optimal_threshold_mv + _ymax/10,f\"baseline std: {1000 * baseline_std:.2f} mV\")\n",
    "#         axes[0,0].set_ylabel(\"Voltage [mV]\",fontsize=fontsize)\n",
    "#         axes[0,0].set_xlabel(\"ADC sample [ns]\",fontsize=fontsize)\n",
    "#         # plot intergral window\n",
    "#         axes[0,0].fill_betweenx([200,_ymax], integral_window[0]*np.max(time), integral_window[1]*np.max(time), color='gray', alpha=0.5)\n",
    "\n",
    "#         axes[0,1].set_xlim(0,3900)\n",
    "#         axes[0,1].set_ylim(-5,1600)\n",
    "#         axes[0,1].set_ylabel(\"Voltage [mV]\",fontsize=fontsize)\n",
    "#         axes[0,1].set_xlabel(\"ADC sample [ns]\",fontsize=fontsize)\n",
    "        \n",
    "#         _ymax = 350\n",
    "#         axes[1,0].set_xlim(0,3900)\n",
    "#         axes[1,0].set_ylim(200,_ymax)\n",
    "#         axes[1,0].text(100,optimal_threshold_mv + _ymax/20,f\"baseline mean: {1000 * baseline_mean:.1f} mV\")\n",
    "#         axes[1,0].text(100,optimal_threshold_mv + _ymax/40,f\"baseline std: {1000 * baseline_std:.2f} mV\")\n",
    "#         axes[1,0].set_ylabel(\"Voltage [mV]\",fontsize=fontsize)\n",
    "#         axes[1,0].set_xlabel(\"ADC sample [ns]\",fontsize=fontsize)\n",
    "#         # plot intergral window\n",
    "#         axes[1,0].fill_betweenx([200,_ymax], integral_window[0]*np.max(time), integral_window[1]*np.max(time), color='gray', alpha=0.5)\n",
    "#         axes[1,0].axhline(1000 * baseline_mean, color=\"gray\",linestyle=\"dashed\",label=f\"Baseline mean\", zorder=10)\n",
    "\n",
    "\n",
    "#         _ymax = 40\n",
    "#         axes[1,1].set_xlim(0,3900)\n",
    "#         axes[1,1].set_ylim(-5,_ymax)\n",
    "#         axes[1,1].set_ylabel(\"Voltage [mV]\",fontsize=fontsize)\n",
    "#         axes[1,1].set_xlabel(\"ADC sample [ns]\",fontsize=fontsize)\n",
    "\n",
    "#         hist,bin_edges,_ = axes[2,0].hist(areas,bins=200,range=(-0.1,10),histtype='step',color='red')\n",
    "#         axes[2,0].set_yscale(\"log\")\n",
    "#         axes[2,0].set_ylabel(\"Count\",fontsize=fontsize)\n",
    "#         axes[2,0].set_xlabel(\"Integrated Area [$V\\cdot ns$]\",fontsize=fontsize)\n",
    "#         axes[2,0].set_ylim(1e-1,None)\n",
    "\n",
    "#         # axes[1,1].hist(heights,bins=100,range=(0,40),histtype='step')\n",
    "#         # axes[2,1].hist2d(areas,heights,bins=[200,100],range=[[-0.1,10],[0,40]],cmap='viridis',norm=\"log\")\n",
    "#         axes[2,1].hist2d(areas,heights,bins=[200,100],cmap='viridis',norm=\"log\")\n",
    "#         axes[2,1].set_ylabel(\"Filtered Height [mV]\",fontsize=fontsize)\n",
    "#         axes[2,1].set_xlabel(\"Filtered integrated area [$V\\cdot ns$]\",fontsize=fontsize)\n",
    "        \n",
    "#         for ax in axes.flatten():\n",
    "#             ax.legend()\n",
    "#         axes[0,0].set_title(f\"Raw WFs channel {channel}\")\n",
    "#         axes[0,1].set_title(f\"Filtered WFs channel {channel}\")\n",
    "\n",
    "\n",
    "#         # if not calibration_run:\n",
    "#         #     ##########finding the peak, and the specifit range###############\n",
    "#         #     hist_diff = np.diff(hist)  # Compute differences between consecutive histogram bins\n",
    "#         #     peaks = np.where((hist_diff[:-1] > 0) & (hist_diff[1:] < 0))[0] + 1  # Find indices where difference changes sign\n",
    "\n",
    "#         #     peak_indices = []\n",
    "#         #     hist_index=[]\n",
    "#         #     for peak_index in peaks:\n",
    "#         #         peak_indices.append(peak_index)\n",
    "#         #         hist_index.append(hist[peak_index])\n",
    "#         #         #axes[2, 0].plot(bin_edges[peak_index], hist[peak_index], marker='o', color='blue', markersize=8, label='Peak')\n",
    "\n",
    "\n",
    "#         #     edge1 = (bin_edges[peak_indices[0]]+bin_edges[peak_indices[1]])/2\n",
    "#         #     edge2 = (bin_edges[peak_indices[1]]+bin_edges[peak_indices[2]])/2\n",
    "#         #     edge3 = (bin_edges[peak_indices[2]]+bin_edges[peak_indices[3]])/2\n",
    "#         #     edge=[edge1,edge2,edge3]\n",
    "        \n",
    "            \n",
    "#         #     bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "#         #     for k in edge:\n",
    "#         #         bin_index = int((k- bin_edges[0]) / bin_width)\n",
    "#         #         corresponding_count = hist[bin_index]\n",
    "#         #         axes[2,0].plot(k, corresponding_count, marker='o', color='blue', markersize=6, label='Peak')\n",
    "            \n",
    "        \n",
    "        \n",
    "#         #     ##########adding the gaussian fit###############\n",
    "#         #     # Define the range of the specific part of the data you want to use for fitting\n",
    "#         #     pe=[]\n",
    "#         #     for j in range(len(edge)-1):\n",
    "#         #         specific_part_range = (edge[j],edge[j+1])\n",
    "#         #         specific_part_areas = [area for area in areas if specific_part_range[0] <= area <= specific_part_range[1]]\n",
    "#         #         mu, sigma = scipy.stats.norm.fit(specific_part_areas)\n",
    "#         #         bins=np.linspace(-0.1, 10, 201)\n",
    "#         #         bins_subset = bins[(bins >= specific_part_range [0]) & (bins <= specific_part_range [1])]\n",
    "                \n",
    "#         #         axes[2, 0].plot(bins_subset, scipy.stats.norm.pdf(bins_subset, mu, sigma), color='green', label='Gaussian Fit')\n",
    "\n",
    "#         #         min_y, max_y = axes[2, 0].get_ylim()\n",
    "#         #         axes[2, 0].vlines(mu, min_y, max_y, color='green', label='Gaussian Fit')\n",
    "\n",
    "#         #         pe.append(mu) # this line has the information of pe1 and pe2 for the gaussian fit\n",
    "\n",
    "#         #     ############ normalize it################\n",
    "#         #     # specific_hist, specific_bin_edges = np.histogram(specific_part_areas, bins=200, range=(edge1, edge2))\n",
    "\n",
    "        \n",
    "#         #     # norm=np.sqrt(sum([i**2 for i in specific_part_areas]))\n",
    "#         #     # normalized=specific_hist/norm\n",
    "\n",
    "#         #     # bin_width = specific_part_areas[1] - specific_part_areas[0]\n",
    "\n",
    "#         #     # # Create the bar plot for the histogram\n",
    "#         #     # axes[2,0].bar(edge2, specific_hist, width=bin_width, edgecolor='black', alpha=0.7)\n",
    "        \n",
    "#         #     #################################################################################################\n",
    "        \n",
    "#         # mu_all_channel.append(pe)\n",
    "\n",
    "\n",
    "#         figure_path = os.path.join(data_folder,\"plot/\")\n",
    "#         if not os.path.exists(figure_path):\n",
    "#                 os.makedirs(figure_path)\n",
    "\n",
    "#         plt.legend(fontsize=\"small\")\n",
    "#         plt.savefig(os.path.join(figure_path,f\"plot_{channel}_{date_datafile}_{time_datafile}.png\"))\n",
    "    \n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_diff = np.diff(hist)  # Compute differences between consecutive histogram bins\n",
    "peaks = np.where((hist_diff[:-1] > 0) & (hist_diff[1:] < 0))[0] + 1  # Find indices where difference changes sign\n",
    "\n",
    "plt.plot(hist)\n",
    "plt.plot(hist_diff)\n",
    "plt.plot(bin_edges[:-1][peaks],hist[peaks],\"o\")\n",
    "\n",
    "# peak_indices = []\n",
    "# hist_index=[]\n",
    "# for peak_index in peaks:\n",
    "#     peak_indices.append(peak_index)\n",
    "#     hist_index.append(hist[peak_index])\n",
    "#     #axes[2, 0].plot(bin_edges[peak_index], hist[peak_index], marker='o', color='blue', markersize=8, label='Peak')\n",
    "\n",
    "\n",
    "# edge1 = (bin_edges[peak_indices[0]]+bin_edges[peak_indices[1]])/2\n",
    "# edge2 = (bin_edges[peak_indices[1]]+bin_edges[peak_indices[2]])/2\n",
    "# edge3 = (bin_edges[peak_indices[2]]+bin_edges[peak_indices[3]])/2\n",
    "# edge=[edge1,edge2,edge3]\n",
    "\n",
    "\n",
    "# bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "# for k in edge:\n",
    "#     bin_index = int((k- bin_edges[0]) / bin_width)\n",
    "#     corresponding_count = hist[bin_index]\n",
    "#     axes[2,0].plot(k, corresponding_count, marker='o', color='blue', markersize=6, label='Peak')\n",
    "\n",
    "\n",
    "\n",
    "# ##########adding the gaussian fit###############\n",
    "# # Define the range of the specific part of the data you want to use for fitting\n",
    "# pe=[]\n",
    "# for j in range(len(edge)-1):\n",
    "#     specific_part_range = (edge[j],edge[j+1])\n",
    "#     specific_part_areas = [area for area in areas if specific_part_range[0] <= area <= specific_part_range[1]]\n",
    "#     mu, sigma = scipy.stats.norm.fit(specific_part_areas)\n",
    "#     bins=np.linspace(-0.1, 10, 201)\n",
    "#     bins_subset = bins[(bins >= specific_part_range [0]) & (bins <= specific_part_range [1])]\n",
    "    \n",
    "#     axes[2, 0].plot(bins_subset, scipy.stats.norm.pdf(bins_subset, mu, sigma), color='green', label='Gaussian Fit')\n",
    "\n",
    "#     min_y, max_y = axes[2, 0].get_ylim()\n",
    "#     axes[2, 0].vlines(mu, min_y, max_y, color='green', label='Gaussian Fit')\n",
    "\n",
    "#     pe.append(mu) # this line has the information of pe1 and pe2 for the gaussian fit\n",
    "# print(\"heyyyyyyyyyyy pe\",pe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Level Time Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/daqtest/DAQ/SandyAQ/softlink_to_data/noise_time_evolution_before_threshold_calibration_implemented/\"\n",
    "# path = \"/home/daqtest/DAQ/SandyAQ/softlink_to_data/*\"\n",
    "calibration_run = False # True if you want to check the calibration, False otherwise\n",
    "plot = False # if want to replot the waveforms\n",
    "ignore_channel_list = np.array([2,8,11,14,20,23])\n",
    "\n",
    "# ##########################\n",
    "# Load the meta data files\n",
    "if calibration_run:\n",
    "    data_folder = os.path.join(path,\"threshold_calibration/\")\n",
    "else:\n",
    "    data_folder = path\n",
    "\n",
    "meta_data_list = glob.glob(f\"{data_folder}/*meta*202405\")\n",
    "#reorganize the order of the meta_data_list, so the final one should be the newest one.\n",
    "\n",
    "# Sort the file paths based on the extracted date and time in ascending order (oldest to newest)\n",
    "new_meta_data_list = sorted(meta_data_list, key=util.extract_date_meta_data)\n",
    "new_meta_data_list = sorted(new_meta_data_list[:], key=util.extract_channel_meta_data)\n",
    "meta_data_list=new_meta_data_list\n",
    "len(meta_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processing = PostProcessing(data_folder, meta_data_list, \n",
    "               ignore_channel_list=ignore_channel_list,\n",
    "               calibration_run=calibration_run)\n",
    "\n",
    "spd = post_processing.process_run(make_plot=False, show_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = post_processing.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_voltage = np.unique(df.bias_voltage.values)\n",
    "df.bias_voltage.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows = df.loc[df['bias_voltage'] == -48.]\n",
    "# plt.plot(rows['datetime_obj'].values, rows['baseline_std'].values, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, (ax1, ax2)= plt.subplots(2,1, figsize=(10, 6))\n",
    "ax1.set_ylabel(\"baseline std\")\n",
    "ax2.set_ylabel(\"baseline mean\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "\n",
    "for voltage in bias_voltage:\n",
    "    rows = df.loc[df['bias_voltage'] == voltage]\n",
    "    ax1.plot(rows['datetime_obj'].values, rows['baseline_std'].values, 'o', label=str(voltage))\n",
    "    \n",
    "    # color_baseline = \"tab:red\"\n",
    "    # ax.plot(datetime_list[mask], baseline_mean_list[mask], 'o--', label=str(voltage), color=color_baseline)\n",
    "    # ax.set_ylabel(\"baseline\", color=color_baseline)\n",
    "    # ax.tick_params(axis='y', labelcolor=color_baseline)\n",
    "\n",
    "    ax2.plot(rows['datetime_obj'].values, rows['baseline_mean'].values, 'o', label=str(voltage))\n",
    "\n",
    "    # color_rms = \"tab:blue\"\n",
    "    # ax2.plot(datetime_list[mask], baseline_std_list[mask], 'o-', label=str(voltage), color=color_rms)\n",
    "    # ax2.set_ylabel(\"baseline std\", color=color_rms)\n",
    "    # ax2.tick_params(axis='y', labelcolor=color_rms)\n",
    "\n",
    "ax1.plot(df['datetime_obj'].values, df['baseline_std'].values, '-')\n",
    "ax2.plot(df['datetime_obj'].values, df['baseline_mean'].values, '--')\n",
    "\n",
    "\n",
    "plt.legend(fontsize=\"small\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, (ax1, ax2)= plt.subplots(2,1, figsize=(10, 6),sharex=True)\n",
    "\n",
    "ax1.set_ylabel(\"baseline std\")\n",
    "ax2.set_ylabel(\"baseline mean\")\n",
    "\n",
    "\n",
    "ax1.set_xlabel(\"threshold\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "\n",
    "for voltage in bias_voltage:\n",
    "    rows = df.loc[df['bias_voltage'] == voltage]\n",
    "    ax1.scatter(rows['threshold_adc'].values.astype(float), rows['baseline_std'].values, marker = 'o', label=str(voltage))\n",
    "    ax2.scatter(rows['threshold_adc'].values.astype(float), rows['baseline_mean'].values, marker = 'o', label=str(voltage))\n",
    "\n",
    "plt.legend(fontsize=\"small\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(numeric_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the config name, and fing the config in the tmp folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = meta_data_list[-3]\n",
    "print(meta_data[0])# makesure it is the newest one\n",
    "meta_data_basename = os.path.basename(meta_data)\n",
    "parts = meta_data_basename.split('_')\n",
    "print(parts)\n",
    "config_name = \"_\".join(parts[1:4]) + \".ini\"\n",
    "threshold_adc = parts[3]\n",
    "channel = int(parts[2])\n",
    "time = int(parts[5].split('.')[0])\n",
    "\n",
    "print(config_name)\n",
    "config_path = os.path.join(data_folder, \"tmp\",config_name)\n",
    "config = configparser.ConfigParser()\n",
    "config.optionxform = str\n",
    "config.read(config_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the processor config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_config = {\"nchs\": 1,\n",
    " \"nsamps\": int(config.get(\"COMMON\", \"RECORD_LENGTH\")),\n",
    " \"sample_selection\": 120,\n",
    " \"samples_to_average\": 40\n",
    "}\n",
    "\n",
    "# dump the config to a json file\n",
    "with open(\"process_config.json\", \"w\") as f:\n",
    "    json.dump(process_config, f)\n",
    "if len(config.get(\"BOARD-0\", \"CHANNEL_LIST\")) > 0:\n",
    "    board_number = 0\n",
    "else:\n",
    "    board_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "a<3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Process waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor= sandpro.processing.rawdata.RawData(config_file = \"process_config.json\",\n",
    "                                               perchannel=False)\n",
    "data_file_basename = meta_data_basename.replace(\"meta_\", \"\").replace(\".json\", f\"_board_{board_number}.bin\")\n",
    "data_file_basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = processor.get_rawdata_numpy(n_evts=4999,\n",
    "                                   file=os.path.join(data_folder, data_file_basename),\n",
    "                                   bit_of_daq=14,\n",
    "                                   headersize=4,inversion=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index, end_index = 1, 4999\n",
    "\n",
    "wfp = WFProcessor(data_folder, volt_per_adc=2/2**14)\n",
    "wfp.set_data(data[\"data_per_channel\"][start_index:end_index,0], in_adc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfp.process_wfs()\n",
    "wfp.plot_random_wfs(5,filtered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = wfp.get_height(search_window=(0.2,0.8))\n",
    "areas = wfp.get_area(sum_window=(0.2,0.8))\n",
    "plt.hist(areas, bins=np.linspace(0, 10, 200))\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adc_to_mv(adc, DCOFFSET=+40, vpp=2.0, bit_of_daq=14):\n",
    "#     #DC OFFSET = +40: -0.2 - 1.8\n",
    "#     #DC OFFSET = +50: 0.0 - 2.0\n",
    "#     #DC OFFSET = -50: -2 - 0.0\n",
    "#     start_voltage = (DCOFFSET/50) -1\n",
    "#     # end_voltage = start_voltage + vpp\n",
    "#     voltage_per_adc = vpp / (2**bit_of_daq-1)\n",
    "#     # return (adc/(2**bit_of_daq-1) * vpp + start_voltage) * 1000\n",
    "#     return (adc * voltage_per_adc + start_voltage) * 1000\n",
    "\n",
    "# def mv_to_adc(mv, DCOFFSET=+40, vpp=2.0, bit_of_daq=14):\n",
    "#     start_voltage = (DCOFFSET/50) -1\n",
    "#     # end_voltage = start_voltage + vpp\n",
    "#     voltage_per_adc = vpp / (2**bit_of_daq-1)\n",
    "#     # return (mv/1000 - start_voltage) / vpp * (2**bit_of_daq-1)\n",
    "#     return (mv/1000 - start_voltage) / voltage_per_adc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data per channel is in volts\n",
    "# Need to convert the threshold to volts\n",
    "\n",
    "# Baseline std and baseline mean are also in volts\n",
    "baseline_std = np.mean(wfp.baseline_rms)\n",
    "baseline_mean = np.mean(wfp.baseline)\n",
    "\n",
    "threshold_mv = util.adc_to_mv(int(threshold_adc))\n",
    "\n",
    "# set the threshold to be 2 * Vpp\n",
    "optimal_threshold_mv = (baseline_mean + 2 * (2 * np.sqrt(2) * baseline_std)) * 1000\n",
    "\n",
    "optimal_threshold_adc = mv_to_adc(optimal_threshold_mv)\n",
    "\n",
    "\n",
    "time = np.arange(0, process_config[\"nsamps\"], 1) * 4\n",
    "for i in range(start_index,end_index):\n",
    "    plt.plot(time, data[\"data_per_channel\"][i][0] * 1000)\n",
    "plt.axhline(optimal_threshold_mv, color='g',label=f\"Proposed threshold: {int(optimal_threshold_adc)} ADC\")\n",
    "plt.axhline(threshold_mv, color='b',label=f\"Test threshold: {int(threshold_adc)} \")\n",
    "plt.text(2500,430,f\"baseline mean: {1000 * baseline_mean:.1f} mV\")\n",
    "plt.text(2500,400,f\"baseline std: {1000 * baseline_std:.2f} mV\")\n",
    "\n",
    "plt.ylim(200,500)\n",
    "plt.xlim(0,3900)\n",
    "plt.xlabel(\"Time (ns)\")\n",
    "plt.ylabel(\"Voltage (mV)\")\n",
    "plt.legend()\n",
    "plt.title(f\"channel {channel}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the area distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = wfp.get_area(sum_window=(0.2,0.5))\n",
    "plt.hist(areas,bins=200,range=(-0.1,1),histtype='step',color='red',label=\"Channel 0\")\n",
    "plt.ylabel(\"Count\",fontsize=14)\n",
    "plt.xlabel(\"Integrated Area [$V\\cdot ns$]\",fontsize=fontsize)\n",
    "#plt.ylim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D array with the specified index sequence\n",
    "array_2d = np.zeros((3, 8))\n",
    "for col in range(8):\n",
    "    array_2d[:, col] = [baseline_mean_list[col*3+1], baseline_mean_list[col*3], baseline_mean_list[col*3+2]]\n",
    "# Plot the 2D array using colormap\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.imshow(array_2d, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Baseline [mV]]')\n",
    "\n",
    "# Add index labels to each cell\n",
    "for i in range(3):\n",
    "    for j in range(8):\n",
    "        index = j * 3 + (1 if i == 0 else 0 if i == 1 else 2)\n",
    "        plt.text(j, i, str(index), ha='center', va='center', color='r')\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(range(8), range(1, 9))\n",
    "plt.yticks(range(3), ['Top', 'Middle', 'Bottom'])\n",
    "plt.title('Mean Baseline for each channel')\n",
    "plt.savefig(\"baseline_mean.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D array with the specified index sequence\n",
    "array_2d = np.zeros((3, 8))\n",
    "for col in range(8):\n",
    "    array_2d[:, col] = [baseline_std_list[col*3+1], baseline_std_list[col*3], baseline_std_list[col*3+2]]\n",
    "# Plot the 2D array using colormap\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.imshow(array_2d, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='RMS [mV]]')\n",
    "\n",
    "# Add index labels to each cell\n",
    "for i in range(3):\n",
    "    for j in range(8):\n",
    "        index = j * 3 + (1 if i == 0 else 0 if i == 1 else 2)\n",
    "        plt.text(j, i, str(index), ha='center', va='center', color='r')\n",
    "\n",
    "\n",
    "\n",
    "plt.xticks(range(8), range(1, 9))\n",
    "plt.yticks(range(3), ['Top', 'Middle', 'Bottom'])\n",
    "plt.title('Noise RMS per channel')\n",
    "plt.savefig(\"baseline_std.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
